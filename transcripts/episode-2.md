Ripensando al tema del rischio, ci rendiamo conto di quanto spesso rimanga qualcosa di astratto. Quando parliamo di rischio, lo colleghiamo sempre a uno scenario: un evento che può accadere, alla cui probabilità associamo un potenziale impatto. Su questo costruiamo i nostri modelli di gestione del rischio. Eppure, nella percezione comune, soprattutto nelle aziende, il rischio rimane qualcosa di nebuloso: “non siamo mai stati attaccati”… almeno per quanto ne sappiamo. Ma non sapere di essere stati attaccati non significa che non sia successo, e nemmeno che non accadrà.

Per questo troviamo fondamentale avere quella che chiamiamo la nostra “prova del nove”: abbiamo costruito una certa infrastruttura, aggiunto livelli di sicurezza, definito un piano di disaster recovery. Sulla carta funziona tutto, nella nostra testa funziona tutto… ma la nostra testa è figlia del nostro contesto, delle nostre abitudini, della nostra bolla. E allora la domanda diventa: abbiamo mai verificato davvero se tutto questo regge sotto attacco?

Ci viene in mente l’esperimento fatto mesi fa sugli EDR: avevamo scritto un semplice malware e l’avevamo testato in parallelo su diversi prodotti. E lì la realtà ci ha colpiti: ciò che ci era stato raccontato non corrispondeva del tutto a ciò che accadeva realmente. Non sempre per incompetenza—spesso è una scelta progettuale, un “by design”. Ma la conseguenza è enorme: scegliere una soluzione che blocca mille scenari rispetto a una che ne blocca solo cento e ne richiede altri duemila da configurare a mano cambia completamente il livello di rischio reale. Eppure, senza un’adeguata competenza tecnica, molte aziende finiscono per scegliere solo sulla base del costo o delle comparazioni da catalogo.

Da qui arriviamo al tema della compliance, che spesso porta a farsi le domande giuste, anche se poi rispondere con sincerità e oggettività non è sempre scontato. Abbiamo visto in più aziende quanto la presenza di una figura esterna renda l’analisi molto più efficace: qualcuno che non trascrive semplicemente le risposte, ma che indaga e mette in dubbio gli assunti.

Passando agli aspetti tecnici, ci soffermiamo sul malinteso più comune riguardo agli EDR: l’idea che “vedano tutto”. Ma detection non significa vedere: significa decidere che cosa mettere in evidenza. È una selezione di eventi, non un unico segnale. Nel nostro esperimento estivo avevamo volutamente scelto una debolezza: alcune attività, in configurazione di default, sono rilevate subito; altre richiedono un contesto più ricco o trigger aggiuntivi. Non è un problema di qualità dell’EDR, ma di gestione. Di configurazione. Di arricchimento. Vale per l’EDR, per i firewall, per gli IDS: qualsiasi tecnologia richiede decisioni e competenze.

Ed è proprio qui che molti SOC vengono ingiustamente accusati: “non hanno visto l’attacco”. Ma la domanda precedente dovrebbe essere: cosa era stato configurato per vedere? Se non ho deciso di registrare o monitorare un certo evento, il SOC non può farci miracoli.

Il nostro punto, insomma, è che le tecnologie funzionano, ma solo se implementate e gestite bene. E i vendor, nelle loro best practice, lo scrivono chiaramente: serve arricchimento, tuning, integrazione continua.

Arriviamo così alla scelta delle tecnologie. Chi opera in aziende manifatturiere, spesso con team IT orientati più al sistemistico che alla sicurezza, raramente ha il tempo o la competenza per valutare nel dettaglio prodotti e vendor. È per questo che le aziende più mature chiedono supporto esterno, qualcuno super partes che possa interpretare differenze, limiti, punti di forza. Nel mondo ideale ci sarebbe un CISO con forte competenza tecnica. Ma dove non c’è, il rischio è fare scelte basate solo sulla carta.

E poi c’è il tema della maturità del vendor. Le startup sono affascinanti, spesso geniali, ma fragili: possono sparire, venire acquistate, cambiare direzione. Alcuni CISO che conosciamo tengono monitorate le startup senza implementarle subito: aspettano che raggiungano un certo livello di maturità, di feedback di mercato, prima di adottarle. Anche perché l’impatto di una scelta sbagliata varia: sbagliare l’antispam è un conto, sbagliare il firewall della tua rete SD-WAN delle 40 sedi è un altro.

Un tema correlato riguarda l’exit strategy, soprattutto nel cloud. Anni fa erano state definite linee guida per le banche: avere sempre la possibilità di uscire da un cloud provider. Ma nella realtà quasi nessuno lo fa davvero. Eppure la domanda è cruciale: cosa succede se il provider fallisce o aumenta i costi in modo insostenibile? L’unico modo per non trovarsi ostaggio è progettare da subito un provisioning basato su Infrastructure as Code, così da potersi muovere — almeno in teoria — più rapidamente.

A questo punto tiriamo le fila: stiamo parlando della necessità crescente di orchestrare le soluzioni di sicurezza. Le aziende hanno accumulato livelli su livelli, spesso mossi da un rischio percepito più emotivamente che misurato. Prima di scegliere le soluzioni bisogna chiedersi come integrarle, come presidiarle. L’obiettivo? Ridurre il costo operativo, evitare strumenti lasciati a sé stessi, e muoversi verso un futuro — forse ancora mitologico — di un singolo pannello di controllo che dia una visione unificata.

In definitiva, l’investimento nella sicurezza ha senso solo se restituisce valore: se configurato male, non riduce il rischio e spreca denaro. E con questo chiudiamo la nostra lunga sessione—quasi un’ora di riflessioni. Ma torneremo: abbiamo già spoilerato i prossimi temi, dalla gestione delle vulnerabilità ai miti dell’EDR, fino al modo in cui formiamo le nostre convinzioni e il vero senso di “conosci te stesso”.

Alla prossima live, sperando di coinvolgere più persone e rispondere anche a qualche domanda in diretta.
