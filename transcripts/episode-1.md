Partiamo con questa prima riflessione sul tema dell’orchestrazione delle soluzioni di sicurezza, un argomento che spesso viene dato per scontato ma che, a guardar bene, nasconde molte complessità. Ci capita di vedere aziende piene di strumenti, tecnologie e piattaforme che non dialogano tra loro. In teoria dovrebbero collaborare per garantire protezione e controllo, ma nella pratica vivono in silos separati, con tutti i pro e i contro che questo comporta.

Quello che notiamo spesso è una tendenza diffusa: di fronte a una nuova esigenza, la prima reazione è acquistare una soluzione che risolva quel problema specifico. Si ragiona poco su come far lavorare insieme le soluzioni già presenti. Così, quando un’azienda ha la maturità e la capacità economica per dotarsi di più strumenti di sicurezza, finisce con l’avere tre o quattro soluzioni – talvolta persino con funzionalità sovrapposte – che non si integrano tra loro. Ogni sistema si comporta come un’entità autonoma, e il risultato è frammentazione, inefficienza e un governo complessivo difficile.

Abbiamo cominciato a ragionare su questo tema diversi anni fa, osservando situazioni dove coesistevano molteplici soluzioni software pensate per la governance o la gestione delle configurazioni. Ognuna faceva bene il proprio lavoro, ma insieme creavano difficoltà evitabili. Per esempio, ogni sistema aveva un proprio database degli asset: non esisteva una fonte unica e centralizzata. L’integrazione a livello di utenza era incoerente, la gestione degli alert frammentata: ogni strumento inviava notifiche a modo suo, costringendo a consultare dashboard diverse per ottenere un quadro completo. Tutte piccole difficoltà che, sommate, portavano a un problema più grande: le soluzioni venivano trascurate, lasciate a se stesse, e finivano per non restituire il valore che dovevano.

Pensiamo al caso classico dell’EDR: genera notifiche in continuazione, ma se nessuno le guarda, siamo punto e a capo. Oppure, peggio ancora, se non siamo sicuri che l’EDR sia installato su tutti gli asset che dovrebbe proteggere, il problema diventa strutturale: abbiamo uno strumento potente che non copre tutto il perimetro. E questo, nella realtà, capita più spesso di quanto si creda. Se avessimo un asset manager correttamente popolato, un EDR potrebbe attingere a quel database e garantire una copertura coerente. Ma spesso la base dati non è aggiornata o è distribuita su più sistemi scollegati.

Dietro a queste difficoltà c’è una questione di metodo. Si tende ad affrontare la sicurezza come somma di strumenti, non come sistema integrato. Si comprano prodotti, si installano, si configurano, ma raramente si pianifica un disegno complessivo. E questo vale sia per la parte infrastrutturale sia per i dati: le informazioni restano frammentate, le piattaforme isolate.

Quando analizziamo le realtà aziendali, vediamo che il problema è trasversale. Non riguarda solo aziende piccole o grandi, né un settore specifico: è sistemico. C’è una tendenza generalizzata a lavorare per compartimenti stagni, anche quando si riconosce l’importanza dell’integrazione.

Ci vengono in mente diversi esempi. Uno di questi riguarda un progetto recente con un cliente che stava trasferendo il proprio datacenter presso un fornitore esterno. Non un cloud pubblico come AWS o Azure, ma un’infrastruttura più “tradizionale”, basata su VMware, Cisco, CI e NSX-T. Il cliente voleva portare con sé il proprio firewall, mantenendo la sua tecnologia – in questo caso Palo Alto – ma il fornitore proponeva di utilizzare il firewall nativo di VMware, quello integrato in NSX-T, per filtrare il traffico est-ovest.

Il problema era evidente: avrebbe avuto due soluzioni parallele, due punti di controllo separati, due sistemi da gestire. Il firewall Palo Alto per il traffico nord-sud e quello NSX-T per l’est-ovest. E queste due soluzioni non si parlavano. Ogni troubleshooting, ogni verifica richiedeva di passare da una console all’altra. La centralizzazione degli alert diventava complicata: gli allarmi uscivano dalla piattaforma XDR di Palo Alto e si perdevano in un contesto esterno difficile da correlare. Ciò che in passato veniva visto come un vantaggio – il famoso “dual bastion” – si è rivelato un contro: disomogeneità, mancanza di integrazione, costi elevati di sviluppo e gestione.

Un altro esempio ci è capitato in un contesto diverso, ma con problematiche simili. Stavamo lavorando con un cliente per implementare un processo di discovery automatica della rete e di messa in ordine dell’asset inventory. Siamo partiti con una mappatura di rete, una “cartina” che mostrava effettivamente tutti i dispositivi connessi. Abbiamo poi confrontato quella lista con l’IPAM del cliente – che al momento era solo un foglio Excel – e successivamente con l’asset inventory ufficiale.

Il risultato è stato sorprendente: circa il 30% degli IP risultava censito nel DNS, ma i restanti due terzi erano completamente sconosciuti. E man mano che ci avvicinavamo alle “strutture dati ufficiali”, le informazioni si diradavano. Meno record, più incertezze. Questo si traduceva in problemi evidenti: EDR non installati dove servivano, vulnerabilità non tracciate, report incompleti. In sostanza, una parte significativa della rete aziendale era invisibile agli strumenti di sicurezza.

Qui tocchiamo un punto importante: la difficoltà di base nel fare discovery. In teoria molte soluzioni di asset inventory hanno già funzioni di discovery integrate, ma nella pratica vengono usate poco o male. Spesso l’implementazione è ridotta, incompleta o non aggiornata. Un buon punto di partenza per capire la situazione reale è osservare la rete: contare i dispositivi effettivamente connessi, sia via cavo che wireless. Basta confrontare il numero di MAC address rilevati con quelli censiti. Se la differenza è ampia, significa che ci sfugge qualcosa. E capita spesso di scoprire il doppio, o addirittura il triplo, degli oggetti che pensavamo di avere.

Dietro tutto questo c’è anche un problema metodologico di governance. Molte organizzazioni non definiscono in modo chiaro che cosa vogliono gestire. Dovremmo cominciare proprio da lì: sedersi a un tavolo e decidere che cosa intendiamo includere nella nostra gestione IT e, di conseguenza, nella nostra sicurezza informatica.

Dovremmo identificare le classi di oggetti da governare – endpoint, server, switch, dispositivi IoT o OT – e partire da quelle. Invece, troppo spesso il processo è invertito: si parte da un censimento parziale, e solo dopo si decide cosa gestire. Questo approccio genera lacune. Perdiamo intere categorie di asset che sfuggono alla governance, come totem informativi, dispositivi embedded, stampanti di rete o macchine industriali. E poi c’è l’enorme capitolo dell’operational technology, un mondo dove questi problemi si amplificano.

In sostanza, la prima difficoltà che incontriamo è che non si ha una visione chiara di ciò che si vuole gestire. Si pensa di avere tutto sotto controllo perché gli endpoint sono nel sistema di ticketing e i server nel cluster VMware. Ma poi una discovery completa rivela che la realtà è molto più ampia. Gestire bene significa sapere su quali asset stiamo esercitando la governance: senza questo, tutto il resto è costruito su fondamenta fragili.

Il passo successivo è fermarsi e fare un’analisi dall’alto. Decidere da dove partire, cosa controllare, come classificare gli oggetti e quale priorità assegnare a ciascuno. Solo dopo possiamo passare alla verifica, confrontando i dati con la realtà. Per i dispositivi interni, la rete è la “verità” più affidabile; per i lavoratori mobili, la verità risiede invece nelle risorse umane, negli elenchi dipendenti, nei dispositivi assegnati. Se una persona riceve lo stipendio, probabilmente ha anche uno strumento aziendale assegnato: un laptop, uno smartphone, un account. Tutti elementi che dovrebbero essere tracciati.

A quel punto si può cominciare un lavoro di correlazione tra fonti diverse: HR, asset inventory, EDR, vulnerability management, sistemi di ticketing. All’inizio le discrepanze saranno numerose, ma questo è un segno di progresso. È proprio da quelle differenze che nasce una comprensione più precisa dell’ambiente.

Naturalmente, tutto questo richiede integrazioni tra sistemi. E qui arriviamo a un altro tema ricorrente: le API. Oggi diamo quasi per scontato che ogni soluzione ne disponga, ma non è sempre così. I prodotti legacy, sviluppati vent’anni fa, spesso non offrono set di API completi. Una piattaforma moderna dovrebbe consentire di fare tutto tramite API; altrimenti è da considerarsi obsoleta. La chimera della “soluzione unica che fa tutto” non esiste. Ogni vendor ha la propria visione, costruisce il proprio ecosistema, e spesso tende a chiuderlo. Le integrazioni tra vendor diversi sono inevitabili, ma vanno progettate e mantenute.

Certo, oggi molti fornitori mettono a disposizione connector e interfacce, ma integrarli richiede sempre effort. E quell’effort deve avere un senso preciso. Non si integrano sistemi “perché il framework lo prevede”: lo si fa se porta un vantaggio tangibile. Ad esempio, integrare un asset management con un sistema di ticketing ha senso se vogliamo associare i case agli asset e misurare quanti incidenti riguardano un certo servizio. Da lì possiamo capire dove investiamo più tempo, quali asset generano più problemi e dove serve intervenire.

Quando le integrazioni non sono pensate con un obiettivo chiaro, diventano solo costi di sviluppo senza ritorno. La chiave è partire da un’esigenza concreta e valutare se l’integrazione genera valore, visibilità o automazione reale.

Se vogliamo capire come siamo arrivati a questa frammentazione, dobbiamo guardare a come le aziende scelgono le tecnologie. Spesso la spinta arriva dall’urgenza. Gli attacchi informatici, le notizie di data breach, i blocchi produttivi: tutto genera pressione. Si avverte la necessità di “fare qualcosa”, e nel tentativo di reagire si acquistano nuove tecnologie – firewall più performanti, EDR di ultima generazione, soluzioni di backup immutabile, piani di disaster recovery. Ogni strato aggiunge sicurezza, ma anche complessità.

Due sono le debolezze principali di questo approccio. Primo: le tecnologie vengono introdotte senza un vero design architetturale, quindi con errori logici di fondo che minano l’efficacia complessiva. Secondo: queste tecnologie non sono presidiate. Si acquistano soluzioni eccellenti, ma nessuno le guarda davvero. Gli alert restano non letti, le dashboard non monitorate, e alla fine la sicurezza resta solo potenziale.

Alla base di tutto c’è una confusione tra “esigenza” e “rischio”. Nel mondo IT tradizionale siamo abituati a ragionare per esigenze: ci serve una funzionalità, cerchiamo la soluzione che la offra. Nel mondo della sicurezza, invece, dovremmo ragionare in termini di rischio: dobbiamo mitigare un rischio specifico, e quindi scegliamo la tecnologia adatta. Ma per farlo bisogna conoscere i propri rischi, non solo quelli raccontati dal mercato.

Molte organizzazioni acquistano strumenti sulla base dei rischi “mainstream”, influenzate da notizie, report e trend di settore. Tutti comprano EDR perché “serve a tutti”, ma magari trascurano il mondo server o OT, dove il rischio reale è più alto. Le decisioni vengono prese su presupposti incompleti, spesso emotivi, e raramente basati su un’analisi di contesto.

Siamo tutti influenzabili, è naturale: leggiamo articoli, seguiamo discussioni su LinkedIn, assorbiamo opinioni e casi di studio. Ma se non guardiamo con attenzione alla nostra realtà, finiamo per applicare soluzioni generiche a problemi specifici. L’unico modo per uscire da questo circolo è costruire un approccio strutturato: capire i rischi reali, definire una governance basata su dati, progettare un’architettura coerente.

Solo così possiamo davvero orchestrare le soluzioni di sicurezza, trasformandole da strumenti isolati a un ecosistema coordinato e capace di restituire valore all’organizzazione.
